import { Router } from 'express';
import { chat, chatStream } from '../controllers/chat.controller';
import { searchLimiter } from '../middleware/rate-limiter.middleware';

const router = Router({ mergeParams: true });

/**
 * POST /v1/companies/:companyId/chat
 *
 * RAG-powered chat endpoint.
 * Retrieves relevant context from vector store and generates an answer using LLM.
 * Supports both regular and streaming responses based on the `stream` parameter.
 *
 * Request body:
 * - query: string (required) - The user's question
 * - messages: ChatMessage[] (optional) - Conversation history
 *
 * System Prompt Options (choose one):
 * - promptTemplate: string (optional) - Use a predefined prompt template:
 *     - 'customer_support' - Default customer service (balanced)
 *     - 'sales_assistant' - Sales-focused with lead generation
 *     - 'technical_support' - Technical documentation and troubleshooting
 *     - 'onboarding_assistant' - New user onboarding
 *     - 'faq_concise' - Brief FAQ-style responses
 *     - 'ecommerce_assistant' - E-commerce product specialist
 * - systemPrompt: string (optional) - Custom system prompt (overrides template)
 *
 * RAG Settings:
 * - limit: number (optional, default: 5) - Number of context chunks to retrieve
 * - rerank: boolean (optional, default: true) - Whether to rerank results
 * - filter: object (optional) - Filter by fileId, fileIds, or projectId
 * - embeddingProvider: 'openai' | 'gemini' (optional) - Embedding provider for RAG
 *
 * LLM Settings:
 * - llmProvider: 'openai' | 'gemini' (optional) - LLM provider to use
 * - maxTokens: number (optional) - Max tokens for response
 * - temperature: number (optional) - LLM temperature (0-2)
 *
 * Response Settings:
 * - includeSources: boolean (optional, default: true) - Include source documents in response
 * - stream: boolean (optional, default: false) - Stream response via SSE
 *
 * Response (non-streaming):
 * - answer: string - The generated answer
 * - sources: ChatSource[] - Source documents used for context
 * - usage: object - Token usage statistics
 * - model: string - Model used
 * - provider: string - Provider used
 *
 * Response (streaming - SSE events):
 * - event: sources - { sources: ChatSource[] }
 * - event: token - { token: string }
 * - event: done - { model, provider, usage }
 * - event: error - { message: string }
 */
router.post('/', searchLimiter, chat);

/**
 * POST /v1/companies/:companyId/chat/stream
 *
 * Dedicated streaming endpoint for RAG-powered chat.
 * Always returns SSE stream regardless of the `stream` parameter.
 *
 * SSE Events:
 * - sources: Sent first with retrieved context chunks
 * - token: Sent for each token generated by the LLM
 * - done: Sent when generation is complete (includes usage stats)
 * - error: Sent if an error occurs during generation
 *
 * Example client usage:
 * ```javascript
 * const eventSource = new EventSource('/v1/companies/:companyId/chat/stream');
 * // Or with fetch:
 * const response = await fetch('/v1/companies/:companyId/chat/stream', {
 *   method: 'POST',
 *   headers: { 'Content-Type': 'application/json' },
 *   body: JSON.stringify({ query: 'What is RAG?' })
 * });
 * const reader = response.body.getReader();
 * // Process SSE events...
 * ```
 */
router.post('/stream', searchLimiter, chatStream);

export default router;
